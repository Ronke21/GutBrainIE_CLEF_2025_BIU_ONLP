{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert predicted entities in ground truth format in the ATLOP format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nltk version: 3.9.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/nlp/ronke21/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "print('Nltk version: {}.'.format(nltk.__version__))\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "from nltk.tokenize import WordPunctTokenizer as wpt\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define path to the prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NER_PREDICTIONS = \"../Predictions/NER/predicted_entities_eval_format.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUTPUT_NER_PREDICTIONS = \"../Train/RE/data/predicted_entities_atlop_format.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input file into a dictionary variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_NER_PREDICTIONS, 'r', encoding='utf-8') as file:\n",
    "\tner_predictions = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokens of length 2 that are not captured by the tokenizer\n",
    "ILLEGAL_WORDS_2 = [').', '(<', '>)', '),', '.,', '].', '],', '.:', '>.', '>,', '))', '+)', '>-', '</', '[<', '-,', '.)', '™,', ')-', '™)', '+.']\n",
    "# Define the tokens of length 3 that are not captured by the tokenizer\n",
    "ILLEGAL_WORDS_3 = ['.),', '.].', '>),', '.).', '>).', ')),', '>)-', '.</']\n",
    "\n",
    "def tokenize_docs(data: dict, data_name: str):\n",
    "\tprint(f\"Tokenizing articles in set {data_name}...\")\n",
    "\n",
    "\tfor pmid, article in data.items():\n",
    "\t\ttitle = article['metadata']['title']\n",
    "\t\tabstract = article['metadata']['abstract']\n",
    "\n",
    "\t\ttitle_spans = list(wpt().span_tokenize(title))\n",
    "\t\tabstract_spans = list(wpt().span_tokenize(abstract))\n",
    "\t\t\n",
    "\t\tarticle['tokenized_title'] = []\n",
    "\t\tfor start, end in title_spans:\n",
    "\t\t\tword = title[start:end]\n",
    "\t\t\tif word in ILLEGAL_WORDS_2:\n",
    "\t\t\t\tword1 = title[start:end-1]\n",
    "\t\t\t\tword2 = title[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_title'].append((word1, start, end-1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word2, end-1, end))\n",
    "\t\t\telif word in ILLEGAL_WORDS_3:\n",
    "\t\t\t\tword1 = title[start:start+1]\n",
    "\t\t\t\tword2 = title[start+1:end-1]\n",
    "\t\t\t\tword3 = title[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_title'].append((word1, start, start+1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word2, start+1, end-1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word3, end-1, end))\n",
    "\t\t\telse:\t\n",
    "\t\t\t\tarticle['tokenized_title'].append((word, start, end))\n",
    "\t\t\n",
    "\t\tarticle['tokenized_abstract'] = []\n",
    "\t\tfor start, end in abstract_spans:\n",
    "\t\t\tword = abstract[start:end]\n",
    "\t\t\tif word in ILLEGAL_WORDS_2:\n",
    "\t\t\t\tword1 = abstract[start:end-1]\n",
    "\t\t\t\tword2 = abstract[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word1, start, end-1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word2, end-1, end))\n",
    "\t\t\telif word in ILLEGAL_WORDS_3:\n",
    "\t\t\t\tword1 = abstract[start:start+1]\n",
    "\t\t\t\tword2 = abstract[start+1:end-1]\n",
    "\t\t\t\tword3 = abstract[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word1, start, start+1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word2, start+1, end-1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word3, end-1, end))\n",
    "\t\t\telse:\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word, start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing articles in set ner_predictions...\n"
     ]
    }
   ],
   "source": [
    "tokenize_docs(ner_predictions, \"ner_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map annotated entities to tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities_to_tokens(data: dict, data_name: str):\n",
    "\tprint(f\"Mapping entities to tokens in set {data_name}...\")\n",
    "\t\n",
    "\tfor pmid, article in data.items():\n",
    "\t\tfor entity in article['entities']:\n",
    "\t\t\tlocation = entity['location']\n",
    "\t\t\tstart = entity['start_idx']\n",
    "\t\t\tend = entity['end_idx']\n",
    "\t\t\tstart_token = None\n",
    "\t\t\tend_token = None\n",
    "\t\t\tif location == 'title':\n",
    "\t\t\t\tfor idx, token in enumerate(article['tokenized_title']):\n",
    "\t\t\t\t\tif start == token[1] and start is not None:\n",
    "\t\t\t\t\t\tstart_token = idx\n",
    "\t\t\t\t\tif end == token[2]-1 and end is not None:\n",
    "\t\t\t\t\t\tend_token = idx\n",
    "\t\t\telif location == 'abstract':\n",
    "\t\t\t\tfor idx, token in enumerate(article['tokenized_abstract']):\n",
    "\t\t\t\t\tif start == token[1] and start is not None:\n",
    "\t\t\t\t\t\tstart_token = idx\n",
    "\t\t\t\t\tif end == token[2]-1 and end is not None:\n",
    "\t\t\t\t\t\tend_token = idx\n",
    "\t\t\telse:\n",
    "\t\t\t\traise Exception(f'{pmid} - Unrecognized Location: {location}')\n",
    "\t\t\tif start_token is not None and end_token is not None:\n",
    "\t\t\t\tentity['start_token'] = start_token\n",
    "\t\t\t\tentity['end_token'] = end_token\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint (data[pmid]['tokenized_title'])\n",
    "\t\t\t\tprint(data[pmid]['tokenized_abstract'])\n",
    "\t\t\t\traise Exception(f'{pmid} - Not able to assign token(s) to entity: {entity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping entities to tokens in set ner_predictions...\n"
     ]
    }
   ],
   "source": [
    "map_entities_to_tokens(ner_predictions, \"ner_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the start and end indices for articles sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define abbreviations to not be splitted by the sentence tokenizer\n",
    "extra_abbrevs = {'etc', 'etc.', 'etc.)', '<i>L', 'sp', 'subsp', '<i>A', '(<i>Hippophae rhamnoides</i> L.)', 'Rupr'}\n",
    "punkt_param = PunktParameters()\n",
    "for abbr in extra_abbrevs:\n",
    "\tpunkt_param.abbrev_types.add(abbr.lower())\n",
    "sentence_splitter = PunktSentenceTokenizer(punkt_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_spans(data: dict, data_name: str):\n",
    "    print(f\"Getting sentence spans in set {data_name}...\")\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        title = article['metadata']['title']\n",
    "        abstract = article['metadata']['abstract']\n",
    "\n",
    "        # Convert the generator to a list so we can iterate it repeatedly.\n",
    "        sentences = list(sentence_splitter.span_tokenize(abstract))\n",
    "\n",
    "        # Prepare a list of booleans that will flag whether the sentence at a given index should be merged with the next one. \n",
    "        # A sentence is merged with the following one if an entity spans across them.\n",
    "        # Initially, no merge is flagged.\n",
    "        merge_next = [False] * len(sentences)\n",
    "        \n",
    "        # Process each entity in the article.\n",
    "        for entity in article['entities']:\n",
    "            location = entity['location']\n",
    "            start = entity['start_idx']\n",
    "            end = entity['end_idx']\n",
    "\n",
    "            # For title entities, we do nothing regarding sentence spans.\n",
    "            if location == 'title':\n",
    "                if end > len(title):\n",
    "                    raise Exception(f'{pmid} - Found title entity having illegal end index: {entity}')\n",
    "                continue\n",
    "\n",
    "            # Only process abstract entities.\n",
    "            if location == 'abstract':\n",
    "                start_sentence = None\n",
    "                end_sentence = None\n",
    "\n",
    "                # Iterate over the original sentence spans to determine in which sentences the entity start and end fall.\n",
    "                for idx, s in enumerate(sentences):\n",
    "                    # Using >= and <= to include boundaries.\n",
    "                    if start >= s[0] and start <= s[1] and start_sentence is None:\n",
    "                        start_sentence = idx\n",
    "                        #print(f'Start sentence assigned: {idx}')\n",
    "                    if end >= s[0] and end <= s[1] and end_sentence is None:\n",
    "                        end_sentence = idx\n",
    "                        #print(f'End sentence assigned: {idx}')\n",
    "\n",
    "                if start_sentence is None:\n",
    "                    raise Exception(f'{pmid} - Start sentence not assigned for entity: {entity}')\n",
    "                if end_sentence is None:\n",
    "                    raise Exception(f'{pmid} - End sentence not assigned for entity: {entity}')\n",
    "                \n",
    "                # If the entity falls in two different sentences, check if they are consecutive.\n",
    "                if start_sentence != end_sentence:\n",
    "                    if end_sentence - start_sentence == 1:\n",
    "                        # Mark that sentence 'start_sentence' should be merged with its following sentence.\n",
    "                        merge_next[start_sentence] = True\n",
    "                        #print(f'{pmid} - Marking merge for sentences {start_sentence} and {end_sentence} due to entity: {entity}')\n",
    "                    else:\n",
    "                        raise Exception(f'{pmid} - Entity assigned to two non-consecutive sentences ({start_sentence}, {end_sentence}): {entity}')\n",
    "        \n",
    "        # At this point, we have a merge flag for each sentence that should be merged with its next one.\n",
    "        # Now we build the updated list of sentence spans, merging as flagged.\n",
    "        new_spans = []\n",
    "        i = 0\n",
    "        while i < len(sentences):\n",
    "            start_val = sentences[i][0]\n",
    "            end_val = sentences[i][1]\n",
    "            # While the current sentence is flagged to merge with the next one, update the end_val.\n",
    "            while i < len(sentences) - 1 and merge_next[i]:\n",
    "                i += 1\n",
    "                end_val = sentences[i][1]\n",
    "            new_spans.append((start_val, end_val))\n",
    "            i += 1\n",
    "\n",
    "        # Add the updated list of sentence spans to the article dictionary.\n",
    "        article['sentences'] = new_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sentence spans in set ner_predictions...\n"
     ]
    }
   ],
   "source": [
    "get_sentence_spans(ner_predictions, \"ner_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if sentence spans have been computed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_spans(data: dict, data_name: str):\n",
    "    print(f\"Checking sentence spans in set {data_name}...\")\n",
    "\n",
    "    for pmid, article in data.items():\n",
    "        # Process each entity in the article.\n",
    "        for entity in article['entities']:\n",
    "            location = entity['location']\n",
    "            start = entity['start_idx']\n",
    "            end = entity['end_idx']\n",
    "            # For title entities, we do nothing regarding sentence spans.\n",
    "            if location == 'title':\n",
    "                continue\n",
    "\n",
    "            # Only process abstract entities.\n",
    "            if location == 'abstract':\n",
    "                start_sentence = None\n",
    "                end_sentence = None\n",
    "\n",
    "                # Iterate over the original sentence spans to determine in which sentences the entity start and end fall.\n",
    "                for idx, s in enumerate(article['sentences']):\n",
    "                    # Using >= and <= to include boundaries.\n",
    "                    if start >= s[0] and start <= s[1] and start_sentence is None:\n",
    "                        start_sentence = idx\n",
    "                        #print(f'Start sentence assigned: {idx}')\n",
    "                    if end >= s[0] and end <= s[1] and end_sentence is None:\n",
    "                        end_sentence = idx\n",
    "                        #print(f'End sentence assigned: {idx}')\n",
    "\n",
    "                if start_sentence is None:\n",
    "                    raise Exception(f'{pmid} - Start sentence not assigned for entity: {entity}')\n",
    "                if end_sentence is None:\n",
    "                    raise Exception(f'{pmid} - End sentence not assigned for entity: {entity}')\n",
    "                \n",
    "                # If the entity falls in two different sentences, raise Exception.\n",
    "                if start_sentence != end_sentence:\n",
    "                      raise Exception(f'{pmid} - Entity assigned to two different sentences ({start_sentence}, {end_sentence}): {entity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking sentence spans in set ner_predictions...\n"
     ]
    }
   ],
   "source": [
    "check_sentence_spans(ner_predictions, \"ner_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map tokens to the sentence in which they are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_sentences(data: dict, data_name: str):\n",
    "    \"\"\"\n",
    "    For each article, map tokens in the 'tokenized abstract' to the sentence in which they are located.\n",
    "    Uses the 'sentences' field in the article, which is assumed to be a list of (start, end) tuples.\n",
    "    \n",
    "    The mapping is stored as a dictionary where the key is the token index (its position in the tokenized abstract)\n",
    "    and the value is the sentence index. For example, if the first token belongs to sentence 0 and the third token\n",
    "    belongs to sentence 1, the mapping will include entries {0: 0, 2: 1}.\n",
    "    \n",
    "    Raises an Exception if a token does not fall within any of the sentence spans.\n",
    "    \"\"\"\n",
    "    print(f\"Mapping tokens to sentences in set {data_name}...\")\n",
    "\n",
    "    for pmid, article in data.items():\n",
    "        # Retrieve the tokenized abstract and the sentence spans.\n",
    "        tokens = article.get('tokenized_abstract')\n",
    "        sentences = article.get('sentences')\n",
    "        \n",
    "        if tokens is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokenized abstract'.\")\n",
    "        if sentences is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'sentences'. Make sure to run get_sentence_spans first.\")\n",
    "        \n",
    "        token_to_sentence = {}\n",
    "        \n",
    "        # Iterate over each token and determine which sentence it belongs to.\n",
    "        for token_index, token_entry in enumerate(tokens):\n",
    "            # Each token_entry is assumed to be a tuple: (token_text, start_offset, end_offset)\n",
    "            token_text, token_start, token_end = token_entry\n",
    "            assigned_sentence = None\n",
    "            \n",
    "            # Check each sentence span to see if the token falls within it.\n",
    "            for sentence_index, (sent_start, sent_end) in enumerate(sentences):\n",
    "                # We assume a token belongs to a sentence if its start is >= sentence start and its end is <= sentence end.\n",
    "                if token_start >= sent_start and token_end <= sent_end:\n",
    "                    assigned_sentence = sentence_index\n",
    "                    break  # Stop once we find the sentence that contains the token.\n",
    "            \n",
    "            if assigned_sentence is None:\n",
    "                raise Exception(\n",
    "                    f\"Token '{token_text}' (index {token_index}, offsets {token_start}-{token_end}) \"\n",
    "                    f\"in article {pmid} does not fall within any sentence span: {sentences}\"\n",
    "                )\n",
    "            \n",
    "            token_to_sentence[token_index] = assigned_sentence\n",
    "        \n",
    "        # Add the mapping to the article dictionary.\n",
    "        article['tokens_to_sentences_map'] = token_to_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping tokens to sentences in set ner_predictions...\n"
     ]
    }
   ],
   "source": [
    "map_tokens_to_sentences(ner_predictions, \"ner_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map entities to the token positions within each sentence containing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities_to_tokens_within_sentences(data: dict, data_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    For each article, this function maps each entity (assumed to be in the abstract)\n",
    "    to the token positions within the sentence that contains it.\n",
    "    \n",
    "    For each entity in article['entities'] (with location 'abstract'), it adds:\n",
    "      - 'located_in_sentence': the sentence index in which the entity's tokens are located,\n",
    "      - 'start_token_in_sentence': the position of the entity's start token within that sentence,\n",
    "      - 'end_token_in_sentence': the position of the entity's end token within that sentence.\n",
    "    \n",
    "    This function relies on:\n",
    "      - article['tokenized abstract']: a list of tokens of the form (token_text, start_offset, end_offset)\n",
    "      - article['tokens_to_sentences_map']: a mapping { token_index -> sentence_index }\n",
    "      - article['sentences']: a list of (start, end) sentence spans for the abstract.\n",
    "    \"\"\"\n",
    "    print(f\"Mapping entities to tokens within sentences in set {data_name}...\")\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        # Retrieve required fields.\n",
    "        tokens = article.get('tokenized_abstract')\n",
    "        token_to_sentence = article.get('tokens_to_sentences_map')\n",
    "        sentences = article.get('sentences')\n",
    "        \n",
    "        if tokens is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokenized abstract'.\")\n",
    "        if token_to_sentence is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokens_to_sentences_map'. Run map_tokens_to_sentences first.\")\n",
    "        if sentences is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'sentences'. Run get_sentence_spans first.\")\n",
    "        \n",
    "        # Build a helper mapping: for each sentence index, list the token indices that fall into that sentence.\n",
    "        sentence_to_token_indices = {}\n",
    "        for token_index in range(len(tokens)):\n",
    "            sent_idx = token_to_sentence.get(token_index)\n",
    "            if sent_idx is None:\n",
    "                raise Exception(\n",
    "                    f\"In article {pmid}, token index {token_index} is not mapped to any sentence. Tokens: {tokens[token_index]}\"\n",
    "                )\n",
    "            sentence_to_token_indices.setdefault(sent_idx, []).append(token_index)\n",
    "        \n",
    "        # Now process each entity.\n",
    "        for entity in article.get('entities', []):\n",
    "            if entity.get('location') != 'abstract': # Only process entities in the abstract.\n",
    "                continue\n",
    "            \n",
    "            # Retrieve the token indices for this entity.\n",
    "            entity_start_token = entity.get('start_token')\n",
    "            entity_end_token = entity.get('end_token')\n",
    "            \n",
    "            if entity_start_token is None or entity_end_token is None:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} is missing start_token or end_token: {entity}\"\n",
    "                )\n",
    "            \n",
    "            # Determine the sentence in which the entity's tokens are located.\n",
    "            sentence_for_start = token_to_sentence.get(entity_start_token)\n",
    "            sentence_for_end = token_to_sentence.get(entity_end_token)\n",
    "            \n",
    "            if sentence_for_start is None or sentence_for_end is None:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} has tokens not mapped to any sentence: {entity}\"\n",
    "                )\n",
    "            \n",
    "            if sentence_for_start != sentence_for_end:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} spans multiple sentences (start in {sentence_for_start}, end in {sentence_for_end}): {entity}\"\n",
    "                )\n",
    "            \n",
    "            located_sentence = sentence_for_start  # or sentence_for_end, both are same.\n",
    "            \n",
    "            # Get the list of token indices for the sentence.\n",
    "            tokens_in_sentence = sentence_to_token_indices.get(located_sentence)\n",
    "            if tokens_in_sentence is None:\n",
    "                raise Exception(\n",
    "                    f\"Sentence {located_sentence} not found in helper mapping for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # Find the position within the sentence for the start token.\n",
    "            try:\n",
    "                start_token_in_sentence = tokens_in_sentence.index(entity_start_token)\n",
    "            except ValueError:\n",
    "                raise Exception(\n",
    "                    f\"Entity start token {entity_start_token} not found in sentence tokens {tokens_in_sentence} for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # And the position within the sentence for the end token.\n",
    "            try:\n",
    "                end_token_in_sentence = tokens_in_sentence.index(entity_end_token)\n",
    "            except ValueError:\n",
    "                raise Exception(\n",
    "                    f\"Entity end token {entity_end_token} not found in sentence tokens {tokens_in_sentence} for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # Add the new fields to the entity.\n",
    "            entity['located_in_sentence'] = located_sentence\n",
    "            entity['start_token_in_sentence'] = start_token_in_sentence\n",
    "            entity['end_token_in_sentence'] = end_token_in_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping entities to tokens within sentences in set ner_predictions...\n"
     ]
    }
   ],
   "source": [
    "map_entities_to_tokens_within_sentences(ner_predictions, \"ner_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert processed annotations to the DocRED format used by ATLOP for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_docred_format(data: dict, data_name: str, is_test=False) -> list:\n",
    "    \"\"\"\n",
    "    Converts articles (in our intermediate format) to the DocRED format.\n",
    "    \n",
    "    For each article, a new dictionary is produced with the following keys:\n",
    "      - \"vertexSet\": a list of entity mentions (each entity becomes a list with one mention).\n",
    "          Each mention is a dict with:\n",
    "              \"pos\": [start_token_in_sentence, end_token_in_sentence],\n",
    "              \"type\": entity label,\n",
    "              \"sent_id\": sentence id (0 for title; abstract sentences are numbered starting at 1),\n",
    "              \"name\": the entity text span.\n",
    "      - \"title\": the title string of the article.\n",
    "      - \"sents\": a list of lists of tokens. The first entry is the title tokenization and subsequent\n",
    "                 entries are the tokenizations of the abstract sentences.\n",
    "    \n",
    "    For abstract sentence tokenization, we use the sentence spans in article['sentences'] (a list of (start, end) offsets)\n",
    "    and the tokenized abstract (article['tokenized abstract'], where each token is a tuple (token_text, start, end)).\n",
    "    \n",
    "    Returns a list of DocRED-formatted document dictionaries.\n",
    "    \"\"\"\n",
    "    print(f\"Converting articles to DocRED format for set {data_name}...\")\n",
    "\n",
    "    docred_docs = []\n",
    "    for pmid, article in data.items():\n",
    "        # 1. Build the vertexSet.\n",
    "        # Each entity becomes a single mention. We assume that the entities in article['entities'] \n",
    "        # are ordered with title entities first, then abstract entities.\n",
    "        vertexSet = []\n",
    "        for entity in article.get('entities', []):\n",
    "            # Determine the sentence id according to DocRED.\n",
    "            # Title entities are assigned to sentence 0.\n",
    "            if entity.get('location') == 'title':\n",
    "                sent_id = 0\n",
    "            else:\n",
    "                # For abstract entities, we expect a field 'located_in_sentence' computed earlier.\n",
    "                # In our intermediate format abstract sentences are numbered 0,1,... but in DocRED the title is sentence 0.\n",
    "                # So add 1.\n",
    "                sent_id = entity.get('located_in_sentence', 0) + 1\n",
    "\n",
    "            # The token offsets of the entity within its sentence.\n",
    "            if entity['location'] == 'title':\n",
    "                pos = [entity.get('start_token'), entity.get('end_token')+1]\n",
    "            else:\n",
    "                pos = [entity.get('start_token_in_sentence'), entity.get('end_token_in_sentence')+1]\n",
    "            mention = {\n",
    "                \"pos\": pos,\n",
    "                \"type\": entity.get(\"label\").upper(),\n",
    "                \"sent_id\": sent_id,\n",
    "                \"name\": entity.get(\"text_span\")\n",
    "            }\n",
    "            # Each vertexSet entry is a list of mentions (we have one per entity).\n",
    "            vertexSet.append([mention])\n",
    "        \n",
    "        # 2. Build the sents field.\n",
    "        # The first sentence is the tokenization of the title.\n",
    "\t\t\n",
    "        title_tokens = article.get(\"tokenized_title\", [])\n",
    "        tokens_in_title = []\n",
    "        for token in title_tokens:\n",
    "            tokens_in_title.append(token[0])\n",
    "        sents = [tokens_in_title]\n",
    "            \n",
    "        # For the abstract sentences, we use article['sentences'] (list of (start, end) spans)\n",
    "        # and article['tokenized abstract'] (list of tokens, each as (token_text, start, end)).\n",
    "        abstract_tokens = article.get(\"tokenized_abstract\", [])\n",
    "        abstract_sents = []\n",
    "        for span in article.get(\"sentences\", []):\n",
    "            s_start, s_end = span\n",
    "            tokens_in_sentence = []\n",
    "            for token, t_start, t_end in abstract_tokens:\n",
    "                # If a token falls completely within the sentence span, add it.\n",
    "                if t_start >= s_start and t_end <= s_end:\n",
    "                    tokens_in_sentence.append(token)\n",
    "            abstract_sents.append(tokens_in_sentence)\n",
    "        sents.extend(abstract_sents)\n",
    "        \n",
    "        # 3. The title string (a concatenation of pmid + '||' + title).\n",
    "        doc_title = f'{pmid}||{article[\"metadata\"][\"title\"]}'\n",
    "        \n",
    "        # 4. Build the final document dictionary.\n",
    "        doc = {\n",
    "            \"vertexSet\": vertexSet,\n",
    "            \"title\": doc_title,\n",
    "            \"sents\": sents\n",
    "        }\n",
    "        docred_docs.append(doc)\n",
    "    \n",
    "    return docred_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting articles to DocRED format for set ner_predictions...\n"
     ]
    }
   ],
   "source": [
    "docred_ner_predictions = convert_to_docred_format(ner_predictions, \"ner_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the dictionary variable to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_to_json(docred_dict, output_file_path):\n",
    "\tdict_with_double_quotes = json.dumps(docred_dict, ensure_ascii=False)\n",
    "\twith open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "\t\tf.write(dict_with_double_quotes)\n",
    "\n",
    "dump_to_json(docred_ner_predictions, PATH_OUTPUT_NER_PREDICTIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gutbrain25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
